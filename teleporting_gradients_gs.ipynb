{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fb0163a-b653-404d-bc9b-6755726c81e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T14:50:23.470111Z",
     "iopub.status.busy": "2022-08-18T14:50:23.464904Z",
     "iopub.status.idle": "2022-08-18T14:50:23.539084Z",
     "shell.execute_reply": "2022-08-18T14:50:23.537258Z",
     "shell.execute_reply.started": "2022-08-18T14:50:23.467358Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1d5bea-3d3a-43c4-94b2-0f6f5eb51f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T14:53:19.635469Z",
     "iopub.status.busy": "2022-08-18T14:53:19.635168Z",
     "iopub.status.idle": "2022-08-18T14:53:20.106585Z",
     "shell.execute_reply": "2022-08-18T14:53:20.105716Z",
     "shell.execute_reply.started": "2022-08-18T14:53:19.635439Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow version 2.7.1.\n",
      "Loaded Fashion-MNIST into x_train, y_train, x_test, y_test.\n",
      "Shapes: x_train: (60000, 28, 28), y_train: (60000, 10), x_test: (10000, 28, 28), y_test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Fashion-MNIST test dataset\n",
    "\n",
    "def load_data():\n",
    "    import tensorflow as tf\n",
    "    print('Using tensorflow version {}.'.format(tf.__version__))\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    # convert labels to categorical samples\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "    print('Loaded Fashion-MNIST into x_train, y_train, x_test, y_test.')\n",
    "    print('Shapes: x_train: {}, y_train: {}, x_test: {}, y_test: {}'.format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))\n",
    "    return ((x_train, y_train), (x_test, y_test))\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "### loading data ###\n",
    "batch_size=32\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d04d66f-8a8e-498d-92ad-ccba3cc926cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T15:14:22.475204Z",
     "iopub.status.busy": "2022-08-14T15:14:22.474623Z",
     "iopub.status.idle": "2022-08-14T15:14:22.610154Z",
     "shell.execute_reply": "2022-08-14T15:14:22.608934Z",
     "shell.execute_reply.started": "2022-08-14T15:14:22.475158Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 16:14:22.496063: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 2 - gradient teleportation net\n",
    "tf.keras.backend.clear_session() # names init\n",
    "inp0 = tf.keras.Input(shape=(1, 3)) # fix shape\n",
    "\n",
    "d1 = tf.keras.layers.Dense(64, activation=\"relu\")(inp0)\n",
    "d2 = tf.keras.layers.Dense(64, activation=\"relu\")(d1)\n",
    "d3 = tf.keras.layers.Dense(64, activation=\"relu\")(d2)\n",
    "d4 = tf.keras.layers.Dense(64, activation=\"relu\")(d3)\n",
    "d5 = tf.keras.layers.Dense(10, activation=\"relu\")(d4)\n",
    "out0 = tf.keras.layers.Softmax()(d5)\n",
    "\n",
    "model = tf.keras.Model([inp0], [out0]) # <- this it the step that defines the training op (way 1 of doing things)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(optimizer=opt, loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d369bd50-533b-45e2-9015-1ee524f1fc5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T15:23:06.664056Z",
     "iopub.status.busy": "2022-08-14T15:23:06.663692Z",
     "iopub.status.idle": "2022-08-14T15:23:06.669133Z",
     "shell.execute_reply": "2022-08-14T15:23:06.667410Z",
     "shell.execute_reply.started": "2022-08-14T15:23:06.664019Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.fit, model.compile define the training cycles - they can be written in a custom way using tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3524009-8a36-4beb-b9f1-45cd1ee5c5cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T16:25:21.473100Z",
     "iopub.status.busy": "2022-08-14T16:25:21.472792Z",
     "iopub.status.idle": "2022-08-14T16:25:21.479787Z",
     "shell.execute_reply": "2022-08-14T16:25:21.478846Z",
     "shell.execute_reply.started": "2022-08-14T16:25:21.473069Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is NOT low level enough for use with teleporting weights.\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True) # self is keras.Model, which does a forward pass model(x)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses) # uses the model's defined loss\n",
    "\n",
    "        trainable_vars = self.trainable_variables # gets the variables which are trainable in the model\n",
    "        gradients = tape.gradient(loss, trainable_vars) # gets all the gradients from the recorded tape\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars)) # applies gradients to trainable_vars to move through\n",
    "        self.compiled_metrics.update_state(y, y_pred) # using the decided metric evaluator, evaluate metrics\n",
    "        return {m.name: m.result() for m in self.metrics} # print in some fancy fashion way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af427d-5e0e-44ac-a4ca-8a5a2c7b47a2",
   "metadata": {},
   "source": [
    "##### implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "856fc324-cf2a-46ee-a697-44b19e159cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T11:22:44.811339Z",
     "iopub.status.busy": "2022-08-22T11:22:44.811024Z",
     "iopub.status.idle": "2022-08-22T11:22:44.838536Z",
     "shell.execute_reply": "2022-08-22T11:22:44.836461Z",
     "shell.execute_reply.started": "2022-08-22T11:22:44.811305Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def repeat_const(tensor, myconst):\n",
    "    shapes = tf.shape(tensor)\n",
    "    return tf.repeat(myconst, shapes[0], axis=0)\n",
    "\n",
    "class Leaky_MergedDense(tf.keras.layers.Layer):\n",
    "    # custom dense layer incorporating w and b in one (helps the teleportation norm op.)\n",
    "    def __init__(self, width, g=None, mat=None):\n",
    "        super(Leaky_MergedDense, self).__init__()\n",
    "        self.width = width\n",
    "        self.shape = width\n",
    "        self.g = g\n",
    "        self.mat = mat\n",
    "        if g is not None:\n",
    "            self.mat /= tf.norm(self.mat)\n",
    "        self.__name__ = 'md'\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.params = self.add_weight(\n",
    "          name=\"params\",\n",
    "          shape=(input_shape[-1]+1, self.width), # changed shape for biases\n",
    "          # paper uses uniform[0, 1] init, but is inconsistent wrt datasets and loss funcs\n",
    "          trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ones = tf.ones((1, 1))\n",
    "        lmbda = tf.keras.layers.Lambda(lambda x: repeat_const(inputs, ones))(inputs)\n",
    "        concatd = tf.keras.layers.Concatenate(axis=1)([inputs, lmbda]) # adds resp. 1s to the input\n",
    "        if self.g == 'prev':\n",
    "            m1 = tf.nn.leaky_relu(tf.matmul(concatd, self.params), alpha=0.01)\n",
    "            m2 = tf.matmul(self.mat, tf.cast(tf.nn.leaky_relu(m1, alpha=0.01)), 'float32')\n",
    "            m3 = tf.matmul(tf.nn.leaky_relu(m2, alpha=1/0.01), tf.linalg.pinv(concatd))\n",
    "            assert m1.shape == m3.shape\n",
    "            return tf.nn.leaky_relu(m3, alpha=0.01)\n",
    "        elif self.g == 'curr':\n",
    "            m1 = tf.matmul(self.params, tf.cast(tf.linalg.pinv(self.mat), 'float32'))\n",
    "            m2 = tf.matmul(concatd, m1)\n",
    "            m3 = tf.nn.leaky_relu(m2, alpha=0.01)\n",
    "            return m3\n",
    "        else:\n",
    "            return tf.nn.leaky_relu(tf.matmul(concatd, self.params), alpha=0.01)\n",
    "\n",
    "# 2 - gradient teleportation net\n",
    "# activations must be bijective\n",
    "tf.keras.backend.clear_session() # names init\n",
    "inp0 = tf.keras.Input(shape=(28, 28)) # fix shape\n",
    "inpf = tf.keras.layers.Flatten()(inp0)\n",
    "\n",
    "d1 = Leaky_MergedDense(64)(inpf)\n",
    "d2 = Leaky_MergedDense(64)(d1)\n",
    "d3 = Leaky_MergedDense(10)(d2)\n",
    "\n",
    "out0 = tf.keras.layers.Softmax()(d3)\n",
    "\n",
    "model = tf.keras.Model([inp0], [out0])\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(optimizer=opt, loss=loss_fn)\n",
    "\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "5f9e3023-b9dc-49ae-b617-49055309225a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T11:22:56.469349Z",
     "iopub.status.busy": "2022-08-22T11:22:56.468780Z",
     "iopub.status.idle": "2022-08-22T11:22:56.485515Z",
     "shell.execute_reply": "2022-08-22T11:22:56.484268Z",
     "shell.execute_reply.started": "2022-08-22T11:22:56.469309Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate invertible teleporters\n",
    "# !x96! <- changeable, preferably by an ascenting scheme\n",
    "\n",
    "import numpy as np\n",
    "def gen_gln(n): # this should be group-specific (ideally SOn) for better results\n",
    "    # generates an invertible nxn matrix\n",
    "    m = np.random.rand(n, n)\n",
    "    mx = np.sum(np.abs(m), axis=1)\n",
    "    np.fill_diagonal(m, mx)\n",
    "    return m/tf.norm(tf.convert_to_tensor(m)) # to ensure size preserving maps\n",
    "\n",
    "def generate_teleport_candidates(matrix_size, num_candidates=10):\n",
    "    # generates num_candidates teleport candidates for use with teleporting gradient descent\n",
    "    return [tf.convert_to_tensor(gen_gln(matrix_size)) for i in range(num_candidates)]\n",
    "# !x96!\n",
    "\n",
    "def model_teleport_update(model, layer_num, mat):\n",
    "    from keras.models import Sequential\n",
    "    new_model = Sequential()\n",
    "    # Go through all layers, if it has a ReLU activation, replace it with PrELU\n",
    "    i = 0\n",
    "    new_model.add(tf.keras.Input(shape=(28, 28)))\n",
    "\n",
    "    for layer in tuple(model.layers):\n",
    "        if hasattr(layer, '__name__'):\n",
    "            i += 1\n",
    "            if layer.__name__ == 'md':\n",
    "                if layer_num-1 == i: # if prev.\n",
    "                    new_layer = Leaky_MergedDense(model.layers[layer_num+2].weights[0].shape[-1], g='prev', mat=mat)\n",
    "                    assert new_layer.shape == model.layers[layer_num+2].shape\n",
    "                    new_model.add(new_layer)\n",
    "                elif layer_num == i: # if curr.\n",
    "                    new_shape = model.layers[layer_num+2].weights[0].shape[-1]\n",
    "                    new_layer = Leaky_MergedDense(new_shape, g='curr', mat=mat)\n",
    "                    assert new_layer.shape == model.layers[layer_num+2].shape\n",
    "                    new_model.add(new_layer)\n",
    "                else:\n",
    "                    new_model.add(layer)\n",
    "        else:\n",
    "            new_model.add(layer)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "35c7874a-dc43-48df-ad8e-8e38829d13ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T00:27:01.752921Z",
     "iopub.status.busy": "2022-08-22T00:27:01.752634Z",
     "iopub.status.idle": "2022-08-22T00:27:01.758302Z",
     "shell.execute_reply": "2022-08-22T00:27:01.757378Z",
     "shell.execute_reply.started": "2022-08-22T00:27:01.752891Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([65, 64]), TensorShape([65, 10]))"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_weights[1].shape, model.trainable_weights[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50e030-7ab2-4381-ace8-d652e8867df5",
   "metadata": {},
   "source": [
    "        `teleport_schedule` decides in which epochs to do teleportations\n",
    "        `teleport_duration` decides the number of teleports to do in the first of the epoch's minibatches\n",
    "        `num_candidates` decides on the number of teleportation candidates to consider - as I am not using gradient ascent here, this is hit or miss so a high number is preferrred\n",
    "        ``\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "2b2f5114-3e78-4453-8ef9-1fd3256b3046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:33:41.992191Z",
     "iopub.status.busy": "2022-08-22T12:33:41.989207Z",
     "iopub.status.idle": "2022-08-22T12:33:42.037866Z",
     "shell.execute_reply": "2022-08-22T12:33:42.036390Z",
     "shell.execute_reply.started": "2022-08-22T12:33:41.992081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# no longer subclassing keras.model directly\n",
    "# here stuff like the opt is defined directly\n",
    "# also true for the loss function, as opposed to being added to the model beforehand\n",
    "# train_dataset = [[x_train[:10000]], [y_train[:10000]]]\n",
    "\n",
    "num_epochs = 40\n",
    "teleport_schedule = {5} # does teleports at this epoch val.\n",
    "                        # tests show one is good enough for the 64, 64, 10 net.\n",
    "teleport_duration = 1 # number of teleports in the first few steps of an epoch\n",
    "\n",
    "def train_nn(model):\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch in teleport_schedule:\n",
    "            #start.epoch\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                if step < teleport_duration:\n",
    "                    ### do teleporting with descent update\n",
    "                    with tf.GradientTape(persistent=True) as tape:\n",
    "                        # expected softmaxed model output\n",
    "                        logits = model(x_batch_train, training=True)\n",
    "                        loss_value = loss_fn(y_batch_train, logits)\n",
    "                    # tracking weights to be gradient descented over (and finding their gradients)\n",
    "                    \n",
    "                    # 1) choose a layer\n",
    "                    # 2) apply GL(n) elements to said weight with m varying instances\n",
    "                    ####### they chose gradient ascent - I'm going it with just random samplings\n",
    "                    # 3) compute m gradient tapes - choose the entry which maximises gradient of said layer\n",
    "                    # 4) update model's prior layer and curr. layer - instantiating it according to teleportation scheme\n",
    "                    # 5) continue training\n",
    "                    \n",
    "                    model_weights = model.trainable_weights\n",
    "                    # this is one way to do teleports - I am simply taking a sampling and teleporting based off the sampling\n",
    "                    # this is different to what was done in the paper - they used gradient ascent here. That one most definitely ends up with better teleports\n",
    "                    # !x96!\n",
    "                    layer_to_teleport = np.random.randint(1, len(model.trainable_weights)-1)\n",
    "                    matrix_size = model.trainable_weights[layer_to_teleport].shape[-1] # this should be the layer outp. dim\n",
    "                    num_candidates=1000\n",
    "                    teleport_candidates = generate_teleport_candidates(matrix_size, num_candidates=num_candidates) # random loss-invariant actions\n",
    "                    # adjusted_layers = [model.trainable_weights[layer_to_teleport]@(tf.cast(tf.linalg.inv(teleport_candidates[i]), 'float32')) for i in range(num_candidates)]\n",
    "                    adjusted_derivs = [tf.linalg.norm(tape.gradient(loss_value, model.trainable_weights)[layer_to_teleport]@tf.cast(tf.linalg.inv(teleport_candidates[i]), 'float32')) for i in range(num_candidates)]\n",
    "                    max_teleport_arg = np.argmax(np.array(adjusted_derivs))\n",
    "                    del tape\n",
    "                    # !x96!\n",
    "                    # change prior layer of net correspondingly to output the correct thing\n",
    "                    # print('model layer count {}, layer to teleport {}'.format(len(model.layers), layer_to_teleport))\n",
    "                    model = model_teleport_update(model, layer_to_teleport, teleport_candidates[max_teleport_arg])\n",
    "                    model.compile(optimizer=optimizer, \n",
    "                                  loss=loss_fn)\n",
    "\n",
    "                    with tf.GradientTape(persistent=True) as tape:\n",
    "                        # expected softmaxed model output\n",
    "                        logits = model(x_batch_train, training=True)\n",
    "                        print('teleporting! (epoch {}, layer {})'.format(epoch, layer_to_teleport))\n",
    "                        loss_value = loss_fn(y_batch_train, logits)\n",
    "                    gradient = tape.gradient(loss_value, model.trainable_weights)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "                else:\n",
    "                    ### do descents as normal\n",
    "                    with tf.GradientTape() as tape: # rec fwd pass\n",
    "                        logits = model(x_batch_train, training=True) # softmaxed model outputs\n",
    "                        loss_value = loss_fn(y_batch_train, logits)\n",
    "                    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "                \n",
    "                train_acc_metric.update_state(y_batch_train, logits)\n",
    "            #end.epoch\n",
    "            \n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "\n",
    "        if epoch not in teleport_schedule: # using SGD\n",
    "            # for each of the training dataset's batch\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(val_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = model(x_batch_train, training=True)\n",
    "                    loss_value = loss_fn(y_batch_train, logits)\n",
    "                gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "                # Update training metric\n",
    "                train_acc_metric.update_state(y_batch_train, logits)\n",
    "            #end.epoch\n",
    "            \n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "        \n",
    "        ### EPOCH METRICS ###\n",
    "        # print training accs and reset states\n",
    "        print(\"Training acc over epoch {}: %.4f\".format(epoch) % (float(train_acc),))\n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "        ### Run a validation loop at the end of each epoch.\n",
    "        for x_batch_val, y_batch_val in [[x_test, y_test]]:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        val_acc = val_acc_metric.result()\n",
    "        val_acc_metric.reset_states()\n",
    "        print(\"Validation acc for epoch {}: %.4f\".format(epoch) % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "7d7d13b5-e53c-4510-b912-3525aae4b107",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-08-22T11:34:53.508623Z",
     "iopub.status.busy": "2022-08-22T11:34:53.508046Z",
     "iopub.status.idle": "2022-08-22T11:39:15.719450Z",
     "shell.execute_reply": "2022-08-22T11:39:15.718710Z",
     "shell.execute_reply.started": "2022-08-22T11:34:53.508579Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc over epoch 0: 0.3490\n",
      "Validation acc for epoch 0: 0.3473\n",
      "Training acc over epoch 1: 0.3470\n",
      "Validation acc for epoch 1: 0.3554\n",
      "Training acc over epoch 2: 0.3631\n",
      "Validation acc for epoch 2: 0.3775\n",
      "Training acc over epoch 3: 0.3833\n",
      "Validation acc for epoch 3: 0.3937\n",
      "Training acc over epoch 4: 0.3962\n",
      "Validation acc for epoch 4: 0.4025\n",
      "teleporting! (epoch 5, layer 1)\n",
      "Training acc over epoch 5: 0.4088\n",
      "Validation acc for epoch 5: 0.5109\n",
      "Training acc over epoch 6: 0.5129\n",
      "Validation acc for epoch 6: 0.5162\n",
      "Training acc over epoch 7: 0.5181\n",
      "Validation acc for epoch 7: 0.5211\n",
      "Training acc over epoch 8: 0.5225\n",
      "Validation acc for epoch 8: 0.5254\n",
      "Training acc over epoch 9: 0.5264\n",
      "Validation acc for epoch 9: 0.5291\n",
      "Training acc over epoch 10: 0.5308\n",
      "Validation acc for epoch 10: 0.5323\n",
      "Training acc over epoch 11: 0.5328\n",
      "Validation acc for epoch 11: 0.5349\n",
      "Training acc over epoch 12: 0.5349\n",
      "Validation acc for epoch 12: 0.5372\n",
      "Training acc over epoch 13: 0.5370\n",
      "Validation acc for epoch 13: 0.5389\n",
      "Training acc over epoch 14: 0.5391\n",
      "Validation acc for epoch 14: 0.5403\n",
      "teleporting! (epoch 15, layer 1)\n",
      "Training acc over epoch 15: 0.1166\n",
      "Validation acc for epoch 15: 0.2450\n",
      "Training acc over epoch 16: 0.2785\n",
      "Validation acc for epoch 16: 0.3265\n",
      "Training acc over epoch 17: 0.3391\n",
      "Validation acc for epoch 17: 0.3516\n",
      "Training acc over epoch 18: 0.3728\n",
      "Validation acc for epoch 18: 0.3949\n",
      "teleporting! (epoch 19, layer 1)\n",
      "Training acc over epoch 19: 0.4170\n",
      "Validation acc for epoch 19: 0.5161\n"
     ]
    }
   ],
   "source": [
    "# g = train_nn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b4e79ad-4b26-4a61-bf08-03a56016e519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T15:22:35.919085Z",
     "iopub.status.busy": "2022-08-14T15:22:35.918507Z",
     "iopub.status.idle": "2022-08-14T15:22:35.923594Z",
     "shell.execute_reply": "2022-08-14T15:22:35.922266Z",
     "shell.execute_reply.started": "2022-08-14T15:22:35.919044Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model, \"dnn.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc7a0b-8751-40f0-b3ff-ab98d735f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2 - running the net\n",
    "# batch_size=source_shape.shape[0]\n",
    "# model.fit(x=[source_shape, target_shape], y=None, epochs=1000, verbose=1, batch_size=batch_size);\n",
    "# plt.plot(model.history.history['loss']);\n",
    "# print(model.history.history['loss'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nets_env",
   "language": "python",
   "name": "nets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
